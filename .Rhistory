meta_data
get_lon_lat <- function(coords_vector, split_char){
# Split all coordinates
splitted <- str_split(coords_vector, split_char)
# Create lon, lat vectors
lon <- c()
lat <- c()
for (i in 1:length(splitted)){
lon <- c(lon, splitted[[i]][1])
lat <- c(lat, splitted[[i]][2])
}
return(cbind(lon, lat))
}
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
char <- "/"
char
rm(char)
meta_data
badewetter_subset
right_join(meta_data, badewetter_subset, by="Station")
badewetter_subset
as_tibble(badewetter_subset)
colnames(badewetter_subset)
strsplit(colnames(badewetter_subset))
strsplit(colnames(badewetter_subset), " ")
strsplit(colnames(badewetter_subset), " ")[[1]]
# join data
col_names <- c("Station", "Time", "Temperatur", "Niederschlag", "Sonnenschein", "Globalstrahlung", "Luftfeuchtigkeit",
"Windgeschwindigkeit")
colnames(badewetter_subset) <- col_names
badewetter_subset
right_join(meta_data, badewetter_subset, by="Station")
badewetter_subset <- ms_data[, subset_cols]
badewetter_subset
test <- right_join(meta_data, badewetter_subset, by="Station")
test
as.data.frame(test)
# convert to data-frame (columns not tibble compatible)
joined <- as.data.frame(joined)
# Join data
joined <- right_join(meta_data, badewetter_subset, by="Station")
# convert to data-frame (columns not tibble compatible)
joined <- as.data.frame(joined)
joined
# Write final table as CSV
write.csv(joined, paste(data_path, "weather_data_joined.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# Write final table as CSV
write.csv(joined, paste(data_path, "weather_data_joined.csv", sep = "/"), row.names = F, na = "-", fileEncoding = "ISO-8859-1")
# libraries
library(tidyverse)
library(data.table)
library(bit64)
# Pathes
# data folder (destination folder)
data_path <- "data"
# Measurements
measurements_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv"
# meta_path <- "data/metadata_wetterstationen.txt"
meta_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/info/VQHA80_de.txt"
# gets metadata_starting column
get_start_line <- function(lines){
# colnames
col_names <- c("stn", "Name", "Länge/Breite", "KM-Koordinaten", "Höhe")
# Get all indizes which match
indizes_per_colname <- list()
for (i in 1:length(col_names)){
indizes_per_colname[[i]] <- grep(col_names[i], lines)
}
# Return the indizes which all have in common
return(Reduce(intersect, indizes_per_colname))
}
# Function returns the line where the metadata-table ends
get_end_line <- function(lines, start_line, lines_expected){
# empty lines used to find break after table
empty_lines <- which(lines == "")
# expected end
exp_end <- start_line + lines_expected
# get index of most probable end-line
# gets the first index which is greated/equal to expected end
end_line <- empty_lines[min(which(empty_lines >= exp_end))]
return(end_line - 1)
}
# Prepare Meta-Data such that it can be processed
# subsets lines and cleans empty lines in-between (if there are any)
get_cleaned_subset <- function(lines, start_line, end_line){
# subset data
line_subset <- lines[start_line:end_line]
# check if there are still empty lines
if(length(which(line_subset == "")) != 0){
empty_lines = which(line_subset == "")
# filter empty lines out
line_subset[-empty_lines]
}
return(line_subset)
}
# read in temporary meta-data
read_meta_correctly <- function(path){
#define fix-width lengths
#define the length of each fixed-width column
lengths <- c(
str_length("stn                                       "),
str_length("Name                         "),
str_length("Länge/Breite                              "),
str_length("KM-Koordinaten                            "),
str_length("Höhe")
)
# Colnames within dataset (hard-coded = must be known in advance!)
col_names <- c("Station", "Name", "Länge/Breite", "Koordinaten", "Höhe")
# read-in metadata
data <- read_fwf(path, col_positions = fwf_widths(lengths, col_names = col_names),
trim_ws = T, skip = 2, locale = locale(encoding = "ISO-8859-1"))
return(data)
}
# Separate Lon, Lat values
# returns matrix with columns: lon, lat
get_lon_lat <- function(coords_vector, split_char){
# Split all coordinates
splitted <- str_split(coords_vector, split_char)
# Create lon, lat vectors
lon <- c()
lat <- c()
for (i in 1:length(splitted)){
lon <- c(lon, splitted[[i]][1])
lat <- c(lat, splitted[[i]][2])
}
return(cbind(lon, lat))
}
(ms_data <- as.data.frame(fread(measurements_path, na.strings = "-")))
cols_orig <- colnames(ms_data)
cols_new <- c("Station", "Time", "Temperatur (°C)", "Niederschlag (mm)", "Sonnenschein (min)",
"Globalstrahlung (W/m^2)", "Luftfeuchtigkeit (%)", "Taupunkt (°C)", "Windrichtung (°)",
"Windgeschwindigkeit (km/h)", "Böenspitze (km/h)", "Luftdruck auf Stationshöhe (QFE, hPa)",
"Luftdruck auf Meeresniveau (QFF, hPa)", "Luftdruck reduziert auf Meereshöhe mit Standard-Atmosphäre (QNH, hPa)",
"Geopotential 850hPa (gpm)", "Geopotential 700hPa (gpm)", "Windrichtung vekt (°)",
"Windgeschw. Turm (km/h)", "Böenspitze Turm (km/h)", "Lufttemperatur Instr 1 (°C)",
"RH Turm (%)", "Taupunkt Turm (°C)")
# Define shortcut variable names through new Column names
colnames(ms_data) <- cols_new
# Subset required for Badewetter-Index
subset_cols <- cols_new[c(1:7, 10)]
badewetter_subset <- ms_data[, subset_cols]
rm("cols_new", "cols_orig")
!file.exists(paste(data_path, "weather_metadata.csv", sep = "/"))
# FETCH METADATA + PROCESS (if necessary)
# Check if file already exists
if(!file.exists(paste(data_path, "weather_metadata.csv", sep = "/"))){
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
# Get required indizes
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
}else{
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
}
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
lines
# Get required indizes
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
start_line
end_line
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
lines
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
meta_data
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(meta_data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lon_lats")
# libraries
library(tidyverse)
library(data.table)
library(bit64)
# Pathes
# data folder (destination folder)
data_path <- "data"
# Measurements
measurements_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv"
# meta_path <- "data/metadata_wetterstationen.txt"
meta_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/info/VQHA80_de.txt"
# gets metadata_starting column
get_start_line <- function(lines){
# colnames
col_names <- c("stn", "Name", "Länge/Breite", "KM-Koordinaten", "Höhe")
# Get all indizes which match
indizes_per_colname <- list()
for (i in 1:length(col_names)){
indizes_per_colname[[i]] <- grep(col_names[i], lines)
}
# Return the indizes which all have in common
return(Reduce(intersect, indizes_per_colname))
}
# Function returns the line where the metadata-table ends
get_end_line <- function(lines, start_line, lines_expected){
# empty lines used to find break after table
empty_lines <- which(lines == "")
# expected end
exp_end <- start_line + lines_expected
# get index of most probable end-line
# gets the first index which is greated/equal to expected end
end_line <- empty_lines[min(which(empty_lines >= exp_end))]
return(end_line - 1)
}
# Prepare Meta-Data such that it can be processed
# subsets lines and cleans empty lines in-between (if there are any)
get_cleaned_subset <- function(lines, start_line, end_line){
# subset data
line_subset <- lines[start_line:end_line]
# check if there are still empty lines
if(length(which(line_subset == "")) != 0){
empty_lines = which(line_subset == "")
# filter empty lines out
line_subset[-empty_lines]
}
return(line_subset)
}
# read in temporary meta-data
read_meta_correctly <- function(path){
#define fix-width lengths
#define the length of each fixed-width column
lengths <- c(
str_length("stn                                       "),
str_length("Name                         "),
str_length("Länge/Breite                              "),
str_length("KM-Koordinaten                            "),
str_length("Höhe")
)
# Colnames within dataset (hard-coded = must be known in advance!)
col_names <- c("Station", "Name", "Länge/Breite", "Koordinaten", "Höhe")
# read-in metadata
data <- read_fwf(path, col_positions = fwf_widths(lengths, col_names = col_names),
trim_ws = T, skip = 2, locale = locale(encoding = "ISO-8859-1"))
return(data)
}
# Separate Lon, Lat values
# returns matrix with columns: lon, lat
get_lon_lat <- function(coords_vector, split_char){
# Split all coordinates
splitted <- str_split(coords_vector, split_char)
# Create lon, lat vectors
lon <- c()
lat <- c()
for (i in 1:length(splitted)){
lon <- c(lon, splitted[[i]][1])
lat <- c(lat, splitted[[i]][2])
}
return(cbind(lon, lat))
}
(ms_data <- as.data.frame(fread(measurements_path, na.strings = "-")))
cols_orig <- colnames(ms_data)
cols_new <- c("Station", "Time", "Temperatur (°C)", "Niederschlag (mm)", "Sonnenschein (min)",
"Globalstrahlung (W/m^2)", "Luftfeuchtigkeit (%)", "Taupunkt (°C)", "Windrichtung (°)",
"Windgeschwindigkeit (km/h)", "Böenspitze (km/h)", "Luftdruck auf Stationshöhe (QFE, hPa)",
"Luftdruck auf Meeresniveau (QFF, hPa)", "Luftdruck reduziert auf Meereshöhe mit Standard-Atmosphäre (QNH, hPa)",
"Geopotential 850hPa (gpm)", "Geopotential 700hPa (gpm)", "Windrichtung vekt (°)",
"Windgeschw. Turm (km/h)", "Böenspitze Turm (km/h)", "Lufttemperatur Instr 1 (°C)",
"RH Turm (%)", "Taupunkt Turm (°C)")
# Define shortcut variable names through new Column names
colnames(ms_data) <- cols_new
# Subset required for Badewetter-Index
subset_cols <- cols_new[c(1:7, 10)]
badewetter_subset <- ms_data[, subset_cols]
rm("cols_new", "cols_orig")
# FETCH METADATA + PROCESS (if necessary)
# Check if file already exists
if(!file.exists(paste(data_path, "weather_metadata.csv", sep = "/"))){
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
# Get required indizes
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(meta_data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lon_lats")
}else{
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
}
meta_data
# Join data
joined <- right_join(meta_data, badewetter_subset, by="Station")
joined
# convert to data-frame (columns not tibble compatible)
joined <- as.data.frame(joined)
joined
# Write final table as CSV
write.csv(joined, paste(data_path, "weather_data_joined.csv", sep = "/"), row.names = F, na = "-", fileEncoding = "ISO-8859-1")
# libraries
library(tidyverse)
# FETCH METADATA + PROCESS (if necessary)
# Check if file already exists
if(!file.exists(paste(data_path, "weather_metadata.csv", sep = "/"))){
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
# Get required indizes
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(meta_data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lon_lats")
}else{
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
}
# Pathes
# data folder (destination folder)
data_path <- "data"
# FETCH METADATA + PROCESS (if necessary)
# Check if file already exists
if(!file.exists(paste(data_path, "weather_metadata.csv", sep = "/"))){
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
# Get required indizes
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
# Tidy coordinates
lon_lats <- as.vector(as.matrix((as.data.frame(select(meta_data, Koordinaten)))))
lon_lats <- get_lon_lat(lon_lats, "/")
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lon_lats")
}else{
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
}
# TODO: create an executable function out of the whole script
# TODO: if running on Linux: coordination with BASH-Script preferable
# Outfile
outpath <- "data"
# libraries
library(httr)
library(XML)
library(rjson)
library(stringr)
###################################################################################################
# Section 0: Auxiliary Functions
extract_webURL <- function(attribute_containing_URL){
attribute2HTML <- htmlParse(attribute_containing_URL)
# Returns the href-attribute of the given string
return(xpathApply(attribute2HTML, "//a/@href")[[1]][1])
}
# Reads the BAFU-HTML-table of measurements of a station site
# Returns a table
read_station_HTMLtable <- function(url){
# Opening web-page
station_site <- GET(url)
station_site <- htmlParse(station_site, encoding = "utf-8")
station_site <- xmlRoot(station_site)
# Return table
return(readHTMLTable(station_site, which = 1, header = F))
}
# Scrapes current temperature data from river-station
scrape_time_temperature <- function(data_table){
column <- grep("Temper+", unlist(data_table[1,]))
# Time
time = str_extract(data_table[1,column], "[0-9]+:[0-9]+")
# Temperatures
temperatures <- as.numeric(as.vector(data_table[2:4, column]))
# Returns Time,  Letzer Messwert, Mittelwert über letzte 24h, Maximum letzte 24h
return(list(time, temperatures))
}
# Extracts values from string + turns it into numeric
get_attributes_as_numeric <- function(string_with_value){
return(as.numeric(strsplit(string_with_value, " ")[[1]][1]))
}
# Scrapes meta-data from riverstation
scrape_metadata <- function(data_table){
row_height <- grep("Stationshöhe", data_table[,1])
row_catchment <- grep("Grösse des Einzugsgebietes", data_table[,1])
# output
out <- rep(NA, 2)
# Returns Stationshöhe, Grösse des Einzugsgebiets
values <- as.vector(data_table[c(row_height, row_catchment), 2])
for (i in 1:2){
out[i] <-get_attributes_as_numeric(values[i])
}
return(out)
}
# TODO: create an executable function out of the whole script
# TODO: if running on Linux: coordination with BASH-Script preferable
# Outfile
outpath <- "data"
# url
file <- "http://data.geo.admin.ch/ch.bafu.hydroweb-messstationen_temperatur/ch.bafu.hydroweb-messstationen_temperatur_de.json"
# read-in JSON-format
rivers_json <- fromJSON(file=file, simplify = T) # TODO: really necessary to simplify?
river_features <- rivers_json$features
# Updated feature-list
updated_features <- list()
# Add Temperature and Metadata-Information for every feature in data-set
# Loop over all features
print("Feature data fetching initialised...")
print(paste("Total features being processed:", length(river_features)))
for (i in 1:length(river_features)){
# get current feature
feature <- river_features[[i]]
# get current properties
ft_props <- feature$properties
# Get Web-URL
ft_url <- extract_webURL(ft_props$description)
# Get Temperature Values: 1 = current, 2 = mean 24h, 3 = max 24h
time_temperatures <- scrape_time_temperature(read_station_HTMLtable(ft_url))
# Get Metadata: 1 = Stationshöhe (m.ü.M.), 2 = Einzugsgebiet (km^2)
stat_meta <- scrape_metadata(read_station_HTMLtable(ft_url))
# Add the additional elements to the feature properties
# time
ft_props$time <- time_temperatures[[1]]
# Current Temperature
ft_props$current <- time_temperatures[[2]][1]
# Mittlere Temperature 24h
ft_props$mittlereT24h <- time_temperatures[[2]][2]
# Maximale Temperatur 24h
ft_props$maxT24h <- time_temperatures[[2]][3]
# Stationshöhe
ft_props$statHeight <- stat_meta[1]
# Einzugsgebiet
ft_props$catchment <- stat_meta[2]
# Update features
feature$properties <- ft_props
# add updated feature to new feature-list
updated_features[[i]] <- feature
}
# replace features with updated ones
rivers_json$features <- updated_features
# Convert update list to JSON
out_json <- toJSON(rivers_json)
# TODO: could also be other data format
# write Updated JSON
write(out_json, file=paste(outpath, "flussdaten_updated.json", sep = "/"))
