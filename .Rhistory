meta_data <- meta_data %>% select(Station, Name, Höhe) %>%
mutate(Longitude = as.numeric(lon_lats[1]),
Latitude = as.numeric(lon_lats[2]))
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lon_lats")
}else{
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
}
# TODO: create an executable function out of the whole script
# TODO: if running on Linux: coordination with BASH-Script preferable
# Outfile
outpath <- "data"
# libraries
library(httr)
library(XML)
library(rjson)
library(stringr)
###################################################################################################
# Section 0: Auxiliary Functions
extract_webURL <- function(attribute_containing_URL){
attribute2HTML <- htmlParse(attribute_containing_URL)
# Returns the href-attribute of the given string
return(xpathApply(attribute2HTML, "//a/@href")[[1]][1])
}
# Reads the BAFU-HTML-table of measurements of a station site
# Returns a table
read_station_HTMLtable <- function(url){
# Opening web-page
station_site <- GET(url)
station_site <- htmlParse(station_site, encoding = "utf-8")
station_site <- xmlRoot(station_site)
# Return table
return(readHTMLTable(station_site, which = 1, header = F))
}
# Scrapes current temperature data from river-station
scrape_time_temperature <- function(data_table){
column <- grep("Temper+", unlist(data_table[1,]))
# Time
time = str_extract(data_table[1,column], "[0-9]+:[0-9]+")
# Temperatures
temperatures <- as.numeric(as.vector(data_table[2:4, column]))
# Returns Time,  Letzer Messwert, Mittelwert über letzte 24h, Maximum letzte 24h
return(list(time, temperatures))
}
# Extracts values from string + turns it into numeric
get_attributes_as_numeric <- function(string_with_value){
return(as.numeric(strsplit(string_with_value, " ")[[1]][1]))
}
# Scrapes meta-data from riverstation
scrape_metadata <- function(data_table){
row_height <- grep("Stationshöhe", data_table[,1])
row_catchment <- grep("Grösse des Einzugsgebietes", data_table[,1])
# output
out <- rep(NA, 2)
# Returns Stationshöhe, Grösse des Einzugsgebiets
values <- as.vector(data_table[c(row_height, row_catchment), 2])
for (i in 1:2){
out[i] <-get_attributes_as_numeric(values[i])
}
return(out)
}
# TODO: create an executable function out of the whole script
# TODO: if running on Linux: coordination with BASH-Script preferable
# Outfile
outpath <- "data"
# url
file <- "http://data.geo.admin.ch/ch.bafu.hydroweb-messstationen_temperatur/ch.bafu.hydroweb-messstationen_temperatur_de.json"
# read-in JSON-format
rivers_json <- fromJSON(file=file, simplify = T) # TODO: really necessary to simplify?
river_features <- rivers_json$features
# Updated feature-list
updated_features <- list()
# Add Temperature and Metadata-Information for every feature in data-set
# Loop over all features
print("Feature data fetching initialised...")
print(paste("Total features being processed:", length(river_features)))
for (i in 1:length(river_features)){
# get current feature
feature <- river_features[[i]]
# get current properties
ft_props <- feature$properties
# Get Web-URL
ft_url <- extract_webURL(ft_props$description)
# Get Temperature Values: 1 = current, 2 = mean 24h, 3 = max 24h
time_temperatures <- scrape_time_temperature(read_station_HTMLtable(ft_url))
# Get Metadata: 1 = Stationshöhe (m.ü.M.), 2 = Einzugsgebiet (km^2)
stat_meta <- scrape_metadata(read_station_HTMLtable(ft_url))
# Add the additional elements to the feature properties
# time
ft_props$time <- time_temperatures[[1]]
# Current Temperature
ft_props$current <- time_temperatures[[2]][1]
# Mittlere Temperature 24h
ft_props$mittlereT24h <- time_temperatures[[2]][2]
# Maximale Temperatur 24h
ft_props$maxT24h <- time_temperatures[[2]][3]
# Stationshöhe
ft_props$statHeight <- stat_meta[1]
# Einzugsgebiet
ft_props$catchment <- stat_meta[2]
# Update features
feature$properties <- ft_props
# add updated feature to new feature-list
updated_features[[i]] <- feature
}
# replace features with updated ones
rivers_json$features <- updated_features
# Convert update list to JSON
out_json <- toJSON(rivers_json)
# TODO: could also be other data format
# write Updated JSON
write(out_json, file=paste(outpath, "flussdaten_updated.json", sep = "/"))
# libraries
read.csv("data/weather_data_joined.csv")
# libraries
read.csv("data/weather_data_joined.csv", na.strings = "-")
library(tidyverse)
library(data.table)
library(bit64)
# Pathes
# data folder (destination folder)
data_path <- "data"
# Measurements
measurements_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv"
# meta_path <- "data/metadata_wetterstationen.txt"
meta_path <- "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/info/VQHA80_de.txt"
###################################################################################################
# Section 0: Auxiliary Functions
# gets metadata_starting column
get_start_line <- function(lines){
# colnames
col_names <- c("stn", "Name", "Länge/Breite", "KM-Koordinaten", "Höhe")
# Get all indizes which match
indizes_per_colname <- list()
for (i in 1:length(col_names)){
indizes_per_colname[[i]] <- grep(col_names[i], lines)
}
# Return the indizes which all have in common
return(Reduce(intersect, indizes_per_colname))
}
# Function returns the line where the metadata-table ends
get_end_line <- function(lines, start_line, lines_expected){
# empty lines used to find break after table
empty_lines <- which(lines == "")
# expected end
exp_end <- start_line + lines_expected
# get index of most probable end-line
# gets the first index which is greated/equal to expected end
end_line <- empty_lines[min(which(empty_lines >= exp_end))]
return(end_line - 1)
}
# Prepare Meta-Data such that it can be processed
# subsets lines and cleans empty lines in-between (if there are any)
get_cleaned_subset <- function(lines, start_line, end_line){
# subset data
line_subset <- lines[start_line:end_line]
# check if there are still empty lines
if(length(which(line_subset == "")) != 0){
empty_lines = which(line_subset == "")
# filter empty lines out
line_subset[-empty_lines]
}
return(line_subset)
}
# read in temporary meta-data
read_meta_correctly <- function(path){
#define fix-width lengths
#define the length of each fixed-width column
lengths <- c(
str_length("stn                                       "),
str_length("Name                         "),
str_length("Länge/Breite                              "),
str_length("KM-Koordinaten                            "),
str_length("Höhe")
)
# Colnames within dataset (hard-coded = must be known in advance!)
col_names <- c("Station", "Name", "Länge/Breite", "Koordinaten", "Höhe")
# read-in metadata
data <- read_fwf(path, col_positions = fwf_widths(lengths, col_names = col_names),
trim_ws = T, skip = 2, locale = locale(encoding = "ISO-8859-1"))
return(data)
}
# Separate Lon, Lat values
# returns matrix with columns: lon, lat
get_lon_lat <- function(coords_vector, split_char){
# Split all coordinates
splitted <- str_split(coords_vector, split_char)
# Create lon, lat vectors
lon <- c()
lat <- c()
for (i in 1:length(splitted)){
lon <- c(lon, splitted[[i]][1])
lat <- c(lat, splitted[[i]][2])
}
return(cbind(lon, lat))
}
###################################################################################################
# Section 1: Fetch newest measurements of the automated weather stations of MeteoSwiss + prepare for join
# 1.1 Read and Process newest Meteo-Station data
# TODO: correct for UTC-time! (also dependent on current date!) e.g. use today() then determine whether summer or winter
# time to use
(ms_data <- as.data.frame(fread(measurements_path, na.strings = "-")))
cols_orig <- colnames(ms_data)
cols_new <- c("Station", "Time", "Temperatur (°C)", "Niederschlag (mm)", "Sonnenschein (min)",
"Globalstrahlung (W/m^2)", "Luftfeuchtigkeit (%)", "Taupunkt (°C)", "Windrichtung (°)",
"Windgeschwindigkeit (km/h)", "Böenspitze (km/h)", "Luftdruck auf Stationshöhe (QFE, hPa)",
"Luftdruck auf Meeresniveau (QFF, hPa)", "Luftdruck reduziert auf Meereshöhe mit Standard-Atmosphäre (QNH, hPa)",
"Geopotential 850hPa (gpm)", "Geopotential 700hPa (gpm)", "Windrichtung vekt (°)",
"Windgeschw. Turm (km/h)", "Böenspitze Turm (km/h)", "Lufttemperatur Instr 1 (°C)",
"RH Turm (%)", "Taupunkt Turm (°C)")
# Define shortcut variable names through new Column names
colnames(ms_data) <- cols_new
# Subset required for Badewetter-Index
subset_cols <- cols_new[c(1:7, 10)]
badewetter_subset <- ms_data[, subset_cols]
rm("cols_new", "cols_orig")
!file.exists(paste(data_path, "weather_metadata.csv", sep = "/")
)
print("Metadata-File fetching and processing initialised...")
lines <- readLines(meta_path)
start_line <- get_start_line(lines)
expected_lines <- dim(ms_data)[1]
end_line <- get_end_line(lines, start_line, expected_lines)
# Get cleaned subset of lines
lines <- get_cleaned_subset(lines, start_line, end_line)
# Temporary file-storage
write(lines, paste(data_path, "temporary_meta.txt", sep = "/"))
rm("lines", "start_line", "expected_lines", "end_line")
# Read-in temporary-file
meta_data <- read_meta_correctly(paste(data_path, "temporary_meta.txt", sep = "/"))
meta_data
# Tidy data
meta_data <- meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten)[1]),
Latitude = as.numeric(str_split(Koordinaten)[2])) >%>
select(-Koordinaten)
meta_data
meta_data %>% select(Station, Name, Koordinaten, Höhe)
str_split(meta_data["Koordinaten"])
str_split(meta_data["Koordinaten"], "/")
meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten, "/")[1]),
Latitude = as.numeric(str_split(Koordinaten, "/")[2]))
meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten, "/")[[1]]),
Latitude = as.numeric(str_split(Koordinaten, "/")[[2]]))
test_coord = meta_data["Koordinaten"][1,]
test_coord
str_split(test_coord)
str_split(test_coord, "/")
str_split(test_coord, "/")[[1]]
meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten, "/")[[1]][1]),
Latitude = as.numeric(str_split(Koordinaten, "/")[[1]][2]))
rm(test_coord)
meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten, "/")[[1]][1]),
Latitude = as.numeric(str_split(Koordinaten, "/")[[1]][2])) %>%
select(-Koordinaten)
meta_data <- meta_data %>% select(Station, Name, Koordinaten, Höhe) %>%
mutate(Longitude = as.numeric(str_split(Koordinaten, "/")[[1]][1]),
Latitude = as.numeric(str_split(Koordinaten, "/")[[1]][2])) %>%
select(-Koordinaten)
# write processed meta-data
write.csv(meta_data, paste(data_path, "weather_metadata.csv", sep = "/"), row.names = F, fileEncoding = "ISO-8859-1")
# remove temporary file
file.remove(paste(data_path, "temporary_meta.txt", sep = "/"))
meta_data <- read_csv(paste(data_path, "weather_metadata.csv", sep = "/"), locale = locale(encoding = "ISO-8859-1"))
meta_data
rm(meta_data)
rm(list=ls())
###################################################################################################
# Section 1: Calculation of the Badewetter-Index
joined_data <- read.csv("data/weather_data_joined.csv", na.strings = "-")
joined_data
# libraries
source("index_auxiliary.R")
typeof(joined_data)
dim(joined_data)
as.data.frame(joined_data)
colnames(joined_data)
?read.csv
# Read-in data
joined_data <- read.csv("data/weather_data_joined.csv", na.strings = "-", encoding = "ISO-8859-1")
joined_data
# Read-in data
joined_data <- read.csv("data/weather_data_joined.csv", na.strings = "-", encoding = "ISO-8859-1", header = T)
joined_data
# Read-in data
joined_data <- read.csv("data/weather_data_joined.csv", na.strings = "-", encoding = "ISO-8859-1", header = T, check.names = T)
joined_data
# get dimensions r-rows, c-cols
dims <- dim(joined_data)
r <- dims[1]
c <- dims[2]
r
c
# Badewetter-Index Calculation
index <- rep(NA, r)
# colnames
col_names <- colnames(joined)
# colnames
col_names <- colnames(joined_data)
col_names
i = 1
temp <- as.numeric(joined[i,grep("Temp+", cols)])
temp <- as.numeric(joined_data[i,grep("Temp+", col_names)])
temp
joined_data
print(temp)
prec <- as.numeric(joined_data[i,grep("Nieder+", col_names)])
sun <- as.numeric(joined_data[i,grep("Sonnen+", col_names)])
glob <- as.numeric(joined_data[i,grep("Global+", col_names)])
feu <- as.numeric(joined_data[i,grep("Luftfeu+", col_names)])
wind <- as.numeric(joined_data[i,grep("Windgesch+", col_names)])
if(!(is.na(temp) || is.na(prec) ||is.na(sun) ||is.na(glob) ||is.na(feu) ||is.na(wind))){
#index[i] <- 0.4*temp + 0.2*prec + 0.05*sun + 0.05*glob + 0.15*feu + 0.15*wind
index[i] <- temp_cont(temp, temp_wgt, temp_max) + prec_cont(prec, prec_wgt) + sun_cont(sun, sun_wgt, sun_max) +
glob_cont(glob, glob_wgt, glob_max) + feu_cont(feu, feu_wgt) + wind_cont(wind, wind_wgt, wind_max)
}
# set-up values
# Settings
# Temperature (degC)
temp_max <-  45
temp_wgt <-  0.4
# Precipitation (mm)
prec_wgt <- 0.2
# Sunshine Durance (min)
sun_max <- 10
sun_wgt <- 0.05
# Global Radiation (W/m^2)
glob_max <- 1000
glob_wgt <- 0.05
# RH / Relative Humidity (%)
feu_wgt <- 0.15
# Wind (km/h)
wind_max <- 25
wind_wgt <- 0.15
if(!(is.na(temp) || is.na(prec) ||is.na(sun) ||is.na(glob) ||is.na(feu) ||is.na(wind))){
#index[i] <- 0.4*temp + 0.2*prec + 0.05*sun + 0.05*glob + 0.15*feu + 0.15*wind
index[i] <- temp_cont(temp, temp_wgt, temp_max) + prec_cont(prec, prec_wgt) + sun_cont(sun, sun_wgt, sun_max) +
glob_cont(glob, glob_wgt, glob_max) + feu_cont(feu, feu_wgt) + wind_cont(wind, wind_wgt, wind_max)
}
index
#maxindex
maxi <- temp_cont(temp_max, temp_wgt, temp_max) + prec_cont(0, prec_wgt) + sun_cont(sun_max, sun_wgt, sun_max) +
glob_cont(glob_max, glob_wgt, glob_max) + feu_cont(90, feu_wgt) + wind_cont(0, wind_wgt, wind_max)
(maxi*100)
#minindex
mini <- temp_cont(-10, temp_wgt, temp_max) + prec_cont(1, prec_wgt) + sun_cont(0, sun_wgt, sun_max) +
glob_cont(0, glob_wgt, glob_max) + feu_cont(0, feu_wgt) + wind_cont(wind_max, wind_wgt, wind_max)
(mini*100)
# BADEWETTER-INDEX CALCULATION
for(i in 1:r){
temp <- as.numeric(joined_data[i,grep("Temp+", col_names)])
print(temp)
prec <- as.numeric(joined_data[i,grep("Nieder+", col_names)])
sun <- as.numeric(joined_data[i,grep("Sonnen+", col_names)])
glob <- as.numeric(joined_data[i,grep("Global+", col_names)])
feu <- as.numeric(joined_data[i,grep("Luftfeu+", col_names)])
wind <- as.numeric(joined_data[i,grep("Windgesch+", col_names)])
if(!(is.na(temp) || is.na(prec) ||is.na(sun) ||is.na(glob) ||is.na(feu) ||is.na(wind))){
#index[i] <- 0.4*temp + 0.2*prec + 0.05*sun + 0.05*glob + 0.15*feu + 0.15*wind
index[i] <- temp_cont(temp, temp_wgt, temp_max) + prec_cont(prec, prec_wgt) + sun_cont(sun, sun_wgt, sun_max) +
glob_cont(glob, glob_wgt, glob_max) + feu_cont(feu, feu_wgt) + wind_cont(wind, wind_wgt, wind_max)
}
}
100/maxi
1/maxi
1/maxi*index
index
# Scale Index up + add to data
index <- as.integer(index*100)
index
joined[,c+1] <- index
joined_data[,c+1] <- index
joined_data
colnames(joined_data)[c+1] <- "Index"
joined_data
col_names <- colnames(joined_data)
temp
prec
prec <- NA
prec
prec_cont(prec, prec_wgt)
###################################################################################################
# Section 2: WRITING END-PRODUCT FILES FOR USE IN QGIS
#Write csv
write.csv(joined_data, "data/badeindex.csv", row.names = F, na = "-", fileEncoding = "ISO-8859-1")
# defining what variable type each column is (required for reading in QGIS)
col_type <- c("\"String\", \"String\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\"")
write(col_type, "data/badeindex.csvt", ncolumns = length(cols))
rm(list=ls()
)
# set-up values
path = "data"
# libraries / functions
source("index_auxiliary.R")
# Read-in data
# TODO: Correct time to UTC+2 (summer) / UTC+1 (winter)
joined_data <- read.csv(paste(path, "weather_data_joined.csv", sep = "/"), na.strings = "-", encoding = "ISO-8859-1", header = T,
check.names = T)
joined_data
paste(path, "badeindex.csv", sep = "/")
paste(path, "badeindex.csvt", sep = "/")
rm(list=ls())
# libraries / functions
source("index_auxiliary.R")
# set-up values
path = "data"
# Settings
# Temperature (degC)
temp_max <-  45
temp_wgt <-  0.4
# Precipitation (mm)
prec_wgt <- 0.2
# Sunshine Durance (min)
sun_max <- 10
sun_wgt <- 0.05
# Global Radiation (W/m^2)
glob_max <- 1000
glob_wgt <- 0.05
# RH / Relative Humidity (%)
feu_wgt <- 0.15
# Wind (km/h)
wind_max <- 25
wind_wgt <- 0.15
prec_cont(NA, prec_wgt)
# Read-in data
# TODO: Correct time to UTC+2 (summer) / UTC+1 (winter)
joined_data <- read.csv(paste(path, "weather_data_joined.csv", sep = "/"), na.strings = "-", encoding = "ISO-8859-1", header = T,
check.names = T)
# get dimensions r-rows, c-cols
dims <- dim(joined_data)
r <- dims[1]
c <- dims[2]
# Badewetter-Index Calculation
index <- rep(NA, r)
col_names <- colnames(joined_data)
# BADEWETTER-INDEX CALCULATION
for(i in 1:r){
temp <- as.numeric(joined_data[i,grep("Temp+", col_names)])
print(temp)
prec <- as.numeric(joined_data[i,grep("Nieder+", col_names)])
sun <- as.numeric(joined_data[i,grep("Sonnen+", col_names)])
glob <- as.numeric(joined_data[i,grep("Global+", col_names)])
feu <- as.numeric(joined_data[i,grep("Luftfeu+", col_names)])
wind <- as.numeric(joined_data[i,grep("Windgesch+", col_names)])
# Index is currently only calculated if all required variables are available
# TODO: could be changed as index will get lower when variables are missing
# BUT ATTENTION: index should not be calculated when crucial variable like temperature or precipitation is missing
# TODO: would have to be adjusted also in auxiliary functions --> condition for when value is NA
if(!(is.na(temp) || is.na(prec))){
#index[i] <- 0.4*temp + 0.2*prec + 0.05*sun + 0.05*glob + 0.15*feu + 0.15*wind
index[i] <- temp_cont(temp, temp_wgt, temp_max) + prec_cont(prec, prec_wgt) + sun_cont(sun, sun_wgt, sun_max) +
glob_cont(glob, glob_wgt, glob_max) + feu_cont(feu, feu_wgt) + wind_cont(wind, wind_wgt, wind_max)
}
}
index
# Possible Index-Range with given specifications for max-variables + weights
# TODO: could / should be standardized such that values are always in the range 0-100
#maxindex
maxi <- temp_cont(temp_max, temp_wgt, temp_max) + prec_cont(0, prec_wgt) + sun_cont(sun_max, sun_wgt, sun_max) +
glob_cont(glob_max, glob_wgt, glob_max) + feu_cont(90, feu_wgt) + wind_cont(0, wind_wgt, wind_max)
(maxi*100)
#minindex
mini <- temp_cont(-10, temp_wgt, temp_max) + prec_cont(1, prec_wgt) + sun_cont(0, sun_wgt, sun_max) +
glob_cont(0, glob_wgt, glob_max) + feu_cont(0, feu_wgt) + wind_cont(wind_max, wind_wgt, wind_max)
(mini*100)
# Standardization
index <- 1/maxi*index
index
# Scale Index up + add to data
index <- as.integer(index*100)
index
joined_data[,c+1] <- index
colnames(joined_data)[c+1] <- "Index"
# update colnames
col_names <- colnames(joined_data)
###################################################################################################
# Section 2: WRITING END-PRODUCT FILES FOR USE IN QGIS
#Write csv
write.csv(joined_data, paste(path, "badeindex.csv", sep = "/"), row.names = F, na = "-", fileEncoding = "ISO-8859-1")
# defining what variable type each column is (required for reading in QGIS)
col_type <- c("\"String\", \"String\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\", \"Real\"")
write(col_type, paste(path, "badeindex.csvt", sep = "/"), ncolumns = length(cols))
!file.exists(paste(path, "badeindex.csvt", sep = "/"))
path
!file.exists(paste(path, "badeindex.csvt", sep = "/"))
rm(list=ls())
library(leaflet)
popup = c("Robin", "Jakub", "Jannes")
leaflet() %>%
addProviderTiles("NASAGIBS.ViirsEarthAtNight2012") %>%
addMarkers(lng = c(-3, 23, 11),
lat = c(52, 53, 49),
popup = popup)
require(sf)
# Reproject all files into the same coordinate system
library(sf)
library(raster)
install.packages("spData")
install.packages("Rtools")
R.version
library(spData)
devtools::install_github("Nowosad/spDataLarge")
install.packages("devtools")
devtools::install_github("Nowosad/spDataLarge")
install.packages("Rtools")
